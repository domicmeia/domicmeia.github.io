---
layout: post
category: Note
use_math: true

---
> `밑바닥부터 시작하는 딥러닝` 정리

단순 정리용으로 작성된 포스트이며 오타가 정말 많습니다.
자세한 내용이 모두 생략되었습니다.

---
{: data-content="start!"}

## Numpy 모듈 : 배열 연산
- 산술 연산(Arithmetic Operations) : 두 배열의 요소(element)간 4칙 연산
   - $+$ : np.add(a,b)
   - $-$ : np.subtract(a,b)
   - $*$ : np.multiply(a,b)
   - / : np.divide(a,b)
- 비교연산(Comparison)
- 배열의 요소별 비교 : 연산 결과를 요소별 Boolean값으로 return

## 모듈 불러오기(import)
- 모듈(module) : 함수나 변수 또는 클래스를 모아 놓은 파일
- import : 파이썬 모듈을 사용할 수 있게 해주는 명령어(현재 디렉터리의 파일 or 파이썬 라이브러리)
- import 모듈 이름
- from 모듈이름 import 모듈함수 : 모듈 이름을 붙이지 않고 함수를 쓸 수 있음
- from 모듈이름 import*

```python
import math
num = math.factorial(5)

from math import factorial, exp
num = factorial(5)
```

## 모듈 : pickle 모듈
- 일반 텍스트를 파일로 저장할 때는 파일 입출력을 이용
- 리스트, 클래스같이 텍스트가 아닌 자료형은 일반적인 입출력 방법으로 할 수 없음
- import pickle : 모든 파이썬 데이터 객체의 입출력 가능
  - wb, rb : 데이터의 입출력은 바이트형식 사용
  - wb : .bin 확장자 사용 가능
- pickle.dump(data, file) : 파일 출력(data를 file로 출력)

```python
import pickle
list = ['a', 'b', 'c']
with open('list.txt', 'wb') as file :
    pickle.dump(list,file) #list 테이터를 list. txt로 출력
```
- 변수 = pickle.load(file) : 파일 입력(한줄씩 읽기)

```python
with open("list.txt", "rb") as file:
    data = pickle.load(file) #한 줄씩 읽음
with open("abc2.bin", "rb") as file:
    data_list = []
    while True:
        try:
            data=pickle.load(file)
        except EOFError:
            Break
        data_list.append(data)
```

## 모듈 : numpy(Numerical Python)모듈
- 대규모 다차원 배열과 행렬 연산 관련 함수를 제공
- np.array(리스트) : 배열 생성
- np.zeoros : 모든 element를 0으로 초기화
- np.ones : 모든 element를 1로 초기화
- np.full : 모든 element를 지정한 "fill_value"로 초기화
- np.eye : 항등 행렬, 대각선 1로 설정
- np.empty : 요소의 초기화 과정에 없이 할당된 메모리 값 그대로 사용
- like : np.zeros_like, np.ones_like ...
- **데이터 생성 함수** : np.linspace, np.arrange, np.logspace
   - np.linspace : start부터 stop 범위에서 num개를 균일한 간격으로 데이터 생성
   - np.arrange : start부터 stop 미만까지 step 간격으로 데이터 생성
   - np.logspace : 로그 스케일로 지정된 범위에서 num 개수만큼 균등 간격으로 데이터 생성

```python
np.linspace(start, stop, num, endpoint, retstep, dtype)

np.logspace(start, stop, num, endpoint, base, dtype)
```
- **난수 기반 배열 생성** : normal, rand, randn, randint, random
  - np.random.normal(mean, std, size): 평균과 표준편차의 정규(가우션) 분포의 데이터 size개 생성
  - np.random.rand(d0,d1,...,dn): shape이 (d0, d1,...,dn)인 배열 생성
     - 난수: [0.1]의 균등 분포(uniform distribution)의 데이터 생성
  - np.random.randn(d0,d1,...,dn)
     - 난수: 표준 정규 분포(Standard Normal Distribution)에서 데이터 생성
  - np.random.randint(low, high, size)
  - np.random.seed(100) : seed 값을 100으로 생성

## 문자열로 객체를 표현할 때 사용하는 **__str__**메서드
  - sys
  - pickle
  - os
  - shutil
  - glob
  - tempfile
  - time
  - calender
  - random
  - webbrowser

## MNIST 데이터 셋
- MNIST 데이터 셋 : 0~9까지 손글씨 숫자 이미지 데이터
- 훈련 이미지 : 6만장
- 시험 이미지 : 1만장
- 회색조 이미지 : 29X28 픽셀 크기(8비트/픽셀)
- load_mnist 메서드 : MNIST 데이터를 읽음(mnist.py 모듈)

## load_mnist 메서드
- 인수 : normalize, flatten, one_hot_label
- normalize 인수 : 이미지 픽셀 값을 0.0~1.0 사이의 값으로 정규화
     - false : 0~255 값(8비트/픽셀)으로 유지함
- flatten 인수 : 이미지를 1차원 배열로 변환함(784,) = 28x28)
     - false : 1X28X28의 3차원 배열 유지함
- one_hot_label : 레이블을 원핫 인코딩 형태로 저장
     - 원핫 인코딩 : [0,0,1,0,0,0]과 같이 요소를 0과 1로 표시
     - false : 숫자 형태의 레이블을 저장

```python
import sys, os #sys.path.append : 경로 이름을 추가
sys.path.append(os.pardir) #부모 디렉터리의 파일을 가져올 수 있도록 설정
from dataset.mnist import load_mnist #mnist.py가 작업 폴더에 있음
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)
#각 데이터 형상 출력
#60,000장의 학습 이미지 데이터
print(x_train.shape) #(60000, 784)
#60,000장의 학습 이미지 데이터에 대한 정답 데이터(label)
print(t_train.shape) #(60000,)
#10,000장의 테스트 이미지 데이터
print(x_test.shape) #(10000, 784)
#10,000장의 테스트 이미지 데이터에 대한 정답 데이터(label)
```

## 클래스
- class
   - 개발자가 직접 클래스를 정의하면 독자적인 자료형을 만들 수 있음
   - 해당 클래스만의 메서드와 속성을 정의할 수 있음
   - 클래스 생성은 딥러닝 모델 생성 시에 필수적
   - __init__이라는 특별한 메서드가 있고, 클래스를 초기화 하는 방법을 정의
       - 초기화용 메서드를 **생성자**, 클래스의 인스턴스가 만들어질 때 한번만 불림
       - 메서드의 첫 번째 인수로 자신을 나타내는 **self**를 명시적으로 씀

```python
class 클래스 이름 :
    def __init__(self, 인수, ...):
    def 메서드 이름 1(self, 인수, ...):
    ...
    def 메스드 이름 n(self, 인수, ...):
```

## 간단한 예제
```python
def __init__(self,name):
    self.name = name       #클래스를 초기화 하는 방법의 정의(생성자, constructor)
                           #self : 자신의 인스턴스, self.name : 인스턴스 변수
                           #self.name = name : nmae 매개 변수를 받아 초기화
```

## 배열
  - 벡터 : 1차원
  - 행렬 : 2차원
  - 텐서 : 3차원
  - 브로드캐스트
      - 넘파이 배열과 스칼라 값의 조합으로 된 산술 연산을 수행
      - 스칼라 값과의 계산이 넘파이 배열의 원소별로 한 번씩 수행됨

## 퍼셉트론
- 신경망(딥러닝)의 기원이 되는 알고리즘
- 1957년 로젠블라트(Frank Rosenblatt)가 고안한 알고리즘

## 퍼셉트론의 한계
- 직선 하나로 영역을 표현할 수 없는 한계
   - 이로인해 비선형 활성화 함수가 필요
- **단층 퍼셉트론은 직선형 영역만 표현할 수 있지만, 다층 퍼섹트론을 사용하면 비선형도 표현할 수 있음**

### 선형(linear)과 비선형(nonlinear)
- 선형 분류
  - 퍼셉트론 출력 : 1 또는 0
  - 활성화 함수 : 계단 함수(step function)

## 신경망
- 신경망
   - 입력층, 은익층(hidden layer), 출력층으로 구성
   - 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 있음

## 활성화 함수
- 입력신호의 총합이 활성화를 일으키는지를 역할
- 퍼셉트론에서는 활성화 함수로 **계단 함수**를 이용한다
- 활성화 함수를 계단 함수에서 다른 함수로 변경한 것이 신경망의 세계로 나아가는 열쇠임
- 신경망에서 자주 이용되는 활성화 함수는 모두 비선형 : 시그모이드 함수, ReLU 함수
   - 신경망에서 선형함수를 사용하면 **층을 깊게(다층 신경망) 하는 의미가 없어지기 때문**

## 계단 함수

$$ f(x) =
\begin{cases}
1 & x > 0 \\
0 & x\leq0
\end{cases} $$

```python
import numpy as np
import matplotlib.pylab as plt

def step_function(x) :
    return np.array(x>0, dtype = np.int)    #x>0 : 1, x<=0 :0

x = np.arrange(-5.0, 5.0, 0.1)    #-5.0~5.0구간을 0.1단위로 데이터 생성
y = step_function(X)              #step 함수 호출
plt.plot(x,y)
plt.ylim(-0.1, 1.1)               #y축의 범위 지정
plt.show()
```
## 활성화 함수 - 시그모이드 함수

$$ g(x) = \frac{1}{1+e^{-x}} $$

```python
import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
    return 1/(1+ np.exp(-x))
def step_function(x):
return np.array(x > 0, dtype=np.int)

x = np.arange(-5.0, 5.0, 0.1)
y1 = sigmoid(x)
y2 = step_function(x)
plt.plot(x, y1)
plt.plot(x, y2, 'k--')
plt.ylim(-0.1, 1.1) #y축 범위 지정
plt.show()
```

## 활성화 함수 - ReLU 함수
- 최근에 신경망 활성화 함수로 주로 사용됨

$$ f(x) =
\begin{cases}
x & x\geq0 \\
0 & x < 0
\end{cases} $$

```python
import numpy as np
import matplotlib.pylab as plt
def relu(x):
return np.maximum(0, x)
x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)
plt.plot(x, y)
plt.ylim(-1.0, 5.5)
plt.show()
```
## 다차원 배열
- 배열의 차원과 크기
  - 차원 : arr.ndim
  - **크기** : arr.shape 행렬의 차원을 표현
  - **arr.reshape** : 배열의 차원을 변경
  - **flatten** : 배열을 1차원 배열로 변환

- 신경망 구현하기
  - 행렬의 곱으로 신경망 계산 수행

## 다차원 배열의 계산
- 신경망의 내적
  - numpy 행렬 연산을 사요하여 신경망을 구현
  - 신경망 : 입력(x)과 가중치(w)의 행렬 곱에 편향(bias,b)을 더한 다음 활성화 함수로 출력

## 출력층 설계하기
- 항등 함수(identity function)와 소프트 맥스 함수(softmax function) 구현하기
- 회귀 : 항등함수
   - 입력 데이터에서 (연속적인) 수치 예측하는 문제
- 다중 분류 : 소프트 맥스 함수
   - 데이터가 어느 분류에 속하느냐는 문제
   - overflow 문제를 막기 위해 계산 시에 입력 신호 중 최댓값을 빼줘야 함
- 이진 분류 : 시그모이드 함수

## 배치
- 입력 데이터를 묶는 것을 의미함
- 배치 처리를 수행함으로써 큰 배열로 이루어진 계산을 하게 되는데, 컴퓨터에서는 큰 배열을 한꺼번에 계산하는 것이 분할된 작은 배열을 여러번 계산하는 것보다 빠름 (즉, **배치 처리를 하면 속도 측면에서 효율적**)

## 신경망 학습
- 학습 : 훈련 데이터(training data)로부터 가중치 매개변수(W)의 최적 값을 구하는 과정
- 오류가 없는 perfect fit을 찾는 것이 아니라
- 손실 함수
  - 신경망의 학습 결과를 훈련 데이터의 레이블(label) 차이를 나타내는 지표
  - *학습은 손실함수의 값을 최대한 작게하는 가중치 매개변수의 최적 값**을 구하는 과정
- SGD(Stochastic Gradient Decent) 알고리즘
  - 손실함수의 값이 작아지게 가중치 매개변수(W) 값을 미분의 기울기 방법으로 찾아가는 알고리즘

## 손실함수
- 평균 제곱 오차
```python
Def mean_squared_error(y, t)
return 0.5*np.sum((y-t)**2)
```
- 교차 엔트로피 오차
```python
Def cross_entropy_error(y, t)
delta = 1e-7
return -np.sum(t * np.log(y+delta))
```

## 왜 손실함수를 사용하는가?
신경망 학습에서는 **최적의 매개변수(가중치 W와 편향 b)를 탐색**할 때
**손실함수의 값을 가능한 작게** 하는 매개변수 값을 찾음

매개변수에 대한 **손실함수의 미분(기울기)**을 계산하고
미분 값을 이용하여 **손실함수 값이 작아지는 방향**으로
**매개변수의 값을 반복하여 업데이트**함
학습의 속도는 learning_rate에 의해 결정됨

**계단함수의 기울기**는 대부분의 구간에서는 0이 되어
신경망 학습에서 **매개변수를 업데이트할 수 없음**
**시그모이드 함수**는 기울기가 0이 되지 않아 신경망이 올바르게 학습됨

## 경사법(Gradient Decent Method)
**기울기**를 이용하여 **함수(손실함수)의 최소값을 찾는 방법**
경사법은 현 위치에서 **기울어진 방향으로 일정거리(learning_rate)만큼 이동**

$$ W_(n+1) = W_n - a(\frac{df}{dx})$$

a= 학습률(learning_rate)
df/dx = 기울기

## 오차역전파법
- 가중치 매개변수의 기울기를 효율적으로 계산하는 방법
- 지금까지 했던 **수치 미분**은 계산 시간이 오래 걸린다는 단점이 있으므로 일반적으로는 **오차역전파법**을 사용함
- 수치 미분은 오차역전파법을 정확히 구현했는지 확인하기 위해 필요

## 순전파
- **계산을 왼쪽에서 오른쪽으로 진행함**
- 즉, 계산 그래프의 출발점부터 종착점으로 전파함

## 역전파
- **계산을 오른쪽에서 왼쪽으로 진행함**
- 역전파가 하는 일은 연쇄법칙의 원리와 같다
- 오차역전파~!

## Affine 계층
- 신경망의 순전파 때 수행하는 **행렬의 내적**은 **기하학에서 어파인 변환(affine transformation)**이라 함
- Affine 계층에서 주의할 점은 변수가 **다차원 배열**이라는 것

## 매개변수 갱신
- 최적화(optimization)
  - 신경망 학습의 목적은 손실 함수(loss function)의 값을 가능한 낮게 매개변수(W,b)를 찾는 것
- 확률적 경사 하강법(SGD)
  - 매개변수에 대해 손실함수의 기울기를 구해 매개변수 값을 갱신하는 일을 반복하여 수행
  - 단순하고 구현도 쉬움

## 확률적 경사 하강법 단점
- 수렴과정에서 갱신마다 기울기가 크게 빠귀는 경우 **shooting(노이즈, Oscillation) 문제
- 시작점에 따라 local minimum으로 수렴하는 문제

## 모멘텀(Momentum)
- 기본적으로 SGD와 유사하지만, 속도라는 개념이 추가됨
- 즉, **물체가 아무런 힘을 받지 않을 때에도 서서히 하강**시키는 역할을 수행함
- 지그재그가 SGD보다 덜함
- **x축 힘은 아주 작지만 방향이 변하지 않아서 한 방향으로 일정하게 가속**
- 거꾸로 y축의 힘은 크지만 위아래로 번갈아 가며 받아서 일정하지 않음
- x축 방향으로 빠르게 다가가 지그재그 움직임이 줄어든다

## 아다그라드(AdaGrad)
- 매개변수 **적용(adaptive) 학습률 조정** 방식
- 기울기에 따라 각 매개변수의 학습률 조정
- **기울기가 큰 매개변수의 학습률을 작게 조정**
- y축 방향은 기울기가 커서 처음에는 크게 움직이지만 큰 기울기에 반비례하여 학습률은 작아짐
- **지그재그 줄어듬**

## 아담(Adam)
- 모멘텀+아다그라드
- deep neural network의 학습에 가장 광범위하게 이용되고 있는 알고리즘

## 어느 갱신 방법이 좋은가?
- 아직까지 모든 문제에서 뛰어난 학습법은 없다

## 가중치 초기값
- 가중치의 초기값을 무엇으로 설정하느냐가 신경망 학습의 성패를 가르는 일이 실제로 자주 발생
- 가중치 감소(Weight decay)
   - 가중치(W) 매개변수의 값이 작아지도록 학습하는 방법
   - 가중치 값을 작게하여 오버피팅을 방지

## 은익중의 활성화 값 분포
- 기울기 소실(gradient vanishing) 문제
- 시그모이드 함수는 출력이 0 또는 1에 가까워지면 기울기가 0이 되므로 데이터가 0과 1에 치우쳐 분포하면 **역전파의 기울기 값이 점점 작아지다 사라지는 문제**

## mnist 데이터 셋으로 본 가중치 초기값 비교

![mnist](./image/mnist.jpg)

## 배치 정규화 알고리즘
- 활성화 함수 출력 값의 분포를 적절하게 조정
- 장점
   - 학습속도 개선
   - 초기값에 크게 의존하지 않음
   - 오버피팅 억제로 드롭아웃 등의 필요성 감소
   - 과적합을 억제함
- 구체적으로 **데이터의 분포가 평균이 0, 분산이 1**이 되는 **정규분포**를 따르도록 정규화를 수행함
- 배치 정규화를 **활성화 함수의 앞 또는 뒤에 삽입함**으로써 데이터의 분포가 덜 치우치게 할 수 있음

## 오버피팅
- 훈련 데이터에만 적응(fitting)하고 시험 데이터에 제대로 적용 못하는 문제
- 매개변수가 많고 표현력이 높은 모델
- 훈련 데이터가 적음

## 가중치 감소(시험 안나옴)
- 과적합은 가중치 매개변수의 값이 커서 발생하는 경우가 많기 때문에, **학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여 과적합을 억제하는 방법

## 드롭 아웃(시험 안나옴)
- 뉴런을 임의로 삭제하면서 학습하는 방법

## 완전 연결 계층의 문제점
- (1,28,28) 이미지를 (784,1) 1차원 이미지로 변환하여 이미지 현상을 이해하지 못함

## 합성곱 계층
- 출력에 가까운 계층은 지금까지 언급된 Affine-ReLU 구성을 사용ㅎㅁ, 마지막 출력 계층에서는 Affine-Softmax 조합을 그대로 사용
- CNN에서는 패딩, 스트라이드 등 CNN 고유 언어를 사용하며, 각 계층 사이에는 **3차원 데이터같이 입체적인 데이터가 흐른다**는 점에서 완전연결 신경망과 다름
- 합성공 계층은 **이미지도 3차원 데이터로 입력받으며 다음 계층에서도 3차원 데이터로 전달**하기 때문에 **형상을 유지**하게 됨
- 이미지 처리와 필터링 기법
  - 이미지에서 **테두리 부분을 추출**하거나 **이미지를 흐릿하게** 만드는 등의 기능을 수행하기 위해 이용
  - 필터는 (FN, C, FH, FW)의 4차원 형상
     - FN은 필터 개수, C는 채널, FH는 필터 높이, FW의 필터 너비
  - 입력 데이터 : im2col로 전개
  - 필터 : reshape를 사용해 2차원 배열로 전개

## 패딩
- 합성곱 연산을 수행하기 전에 **주변 데이터를 0으로 채움**
- 패딩은 주로 출력 크기를 조정할 목적으로 사용됨
- 패딩을 크게 출력하면 출력 크기가 커진다

## 스트라이드
- 스트라이드를 크게 설정하면 출력 크기는 작아짐

## 3차원 데이터의 합성곱 연산
- 입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고 **그 결과를 합함**

## 풀링 계층
- 풀링은 세로, 가로 방향의 공간을 줄이는 연산

## im2col로 데이터 전개하기
- 합성곱 연산 : im2col(image to columm) 사용
- 입력 데이터를 쉽게 필터링하는 함수
- 3차원 입력 데이터를 2차원 행렬로 바꾸어 필터링 연산 수행함

1. 필터를 세로로 1열로 전개
2. im2col이 전개한 입력 데이터와 필터의 행렬 곱을 계산
3. 출력 데이터의 형상을 변환(reshape)

## 폴링 연산 : im2col 사용
- 입력 데이터를 쉽게 필터링하는 함수
- 3차원 입력 데이터를 2차원 행렬로 바꾸어 폴링 연산 수행함

1. 입력 데이터를 전개
2. 행별 최댓값 계산
3. 적절한 형상으로 변환(reshape)